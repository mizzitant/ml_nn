{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning\n",
    "dt.: *Bestärkendes Lernen*\n",
    "\n",
    "Für wen?\n",
    "\n",
    "Schon einfache Computerspiele sind als Umgebung zu komplex um mit einfacher Klassifizierung zu einer brauchbaren Lösung zu kommen. Ursachen:\n",
    "\n",
    "* *supervised*: Jede Trainingsinstanz hat ein Label.\n",
    "  * Sogar in der einfachen Umgebung eines Computerspieles bräuchte man sehr viele Instanzen -> Screenshots für jede Situation/Spielzustand. \n",
    "  * Spielzüge, die sich erst später im Spiel auswirken bleiben unberücksichtigt.\n",
    "* *Reinforcement Learning*: zwischen *supervised* und *unsupervised* *learning*. Manche Zustände bringen *rewards*. Diese Rewards können als (spärliche und zeitversetze) Labels betrachtet werden. Über *Reinforcement Learning* werden Wege dorthin durch Bewertung von Zuständen, die für sich genommen noch keinen Erfolg bringen, gelernt.\n",
    "\n",
    "### Schwierigkeiten:\n",
    "* **credit assignment problem**: Welche der Züge waren für das Bekommen der Belohnung verantwortlich?\n",
    "* **explore-exploit dilemma**: soll eine einmal gefundene, funktionierende Strategie beibehalten werden (*exploitation*), oder ist es zielführend eine nach einer besseren zu suchen (*exploration*)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Markov Decision Process (MDP)\n",
    "\n",
    "Eine übliche Art den Sachverhalt zu beschreiben ist die Betrachtung als *Markov Decision Process*.\n",
    "\n",
    "\n",
    "* Aktionen (***actions***) führen zu einem neuen Zustand (***state***) und können die Umgebung verändern.\n",
    "* *actions* führen manchmal zu Belohnungen durch das *environment* oder einen Interpreter des Zustands\n",
    " \n",
    "\n",
    "<img src=\"images/actor_state_environment.png\" alt=\"[Die Reinforcement Learning Komponenten: Actor State Environment\" style=\"width: 300px;\"/>\n",
    "Die Reinforcement Learning Komponenten: Actor, State, Environment<br>Source: selfmade\n",
    "\n",
    "* Die Regeln zur Wahl einer Aktion werden unter ***policy*** zusammengefasst.\n",
    "* Die Umgebung ist nicht zwingend deterministisch, dh. ausgehend vom gleichen Zustand führt eine Aktion nicht immer zum gleichen Folgezustand (Zufallselemente -> stochastisches Verhalten)\n",
    "* Eine *Episode* ist die Abfolge von *states*, *actions*, *rewards* bis zum Spielende (finaler Zustand $s_n$): $s_0,a_0,r_1, s_1,a_1,r_2, ...s_{n-1},a_{n-1},r_n,s_n$\n",
    "\n",
    "<img src=\"images/markov_decision_process.png\" alt=\"Markov Decision Process Graph\" style=\"width: 300px;\"/>\n",
    "Markov Decision Process Graph<br>Source: Wikipedia\n",
    "\n",
    "* Wichtige Eigenschaft von *MDP*: Der *MDP* basiert auf der Annahme, daß der nächste Zustand $s_{i+1}$ (bzw. die Wahrscheinlichkeit dafür) ausschließlich vom derzeitigen Zustand $s$ und der gewählten Aktion $a$ abhängt, nicht aber von den Zuständen davor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Zukünftige Belohnungen diskontieren\n",
    "\n",
    "Um langfristig gut abzuschneiden müssen zukünftige potentielle Belohnungen miteinbezogen werden.\n",
    "\n",
    "Der *reward* $R$ einer gesamten Episode lässt sich einfach berechnen: $R = r_1 + r_2 +  .. + r_n$, bzw. für den verbleibenden Spielverlauf: $R_t = r_t + r_{t+1} +  .. + r_n$\n",
    "\n",
    "Wie der zukünftige Verlauf ist, lässt sich aber nicht zuverlässig vorhersagen (stochastische Umgebung). Daher ist es üblich die kommende Belohnung nur mit einer bestimmten Wahrscheinlichkeit $\\gamma$ zu berücksichtigen und die darauf folgende Belohnung wieder mit der gleichen Wahrscheinlichkeit, was zu $\\gamma^2$ führt, usw.:\n",
    "\n",
    "### $R_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} +  .. + \\gamma^{n-t}r_n$\n",
    "\n",
    "Dh. die kommenden Belohnungen werden diskontiert. Kompakter dargestellt:\n",
    "\n",
    "### $\\displaystyle R_t = \\sum^{N-1}_{t=0}\\gamma^t\\cdot r_{t+1}$\n",
    "\n",
    "$\\gamma$ ist ein Wert zwischen $0$ und $1$. Dh. je weiter in der Zukunft eine Belohnung ist, desto geringer ist ihr Einfluss. Je näher $\\gamma$ bei 1 ist, desto stärker werden zukünftige Belohnungen berücksichtigt. In einer deterministischen Umgebung kann zB. $\\gamma=1$ gesetzt werden. Wenn $\\gamma=0$ wird nur die unmittelbar nächste Belohnung in Betracht gezogen.\n",
    "\n",
    "### $R_t = r_t + \\gamma(r_{t+1} + \\gamma(r_{t+2} + ...)) = r_t + \\gamma R_{t+1}$\n",
    "\n",
    "Nun gilt es die Aktionen so zu wählen, dass $R_t$ maximiert wird, und hier kommt Q-Learning ins Spiel!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "**$Q(s, a)$** ist der ***discounted future reward***, wenn die Aktion $a$ im Zustand $s$ gesetzt wird und alle **Folgeaktionen optimal gewählt** sind. Woher wissen wir, dass die Aktionen optimal gewählt wurden, oder welche Aktionen optimal sind? Zunächst wissen wir es gar nicht. Aber wir können uns hinarbeiten.\n",
    "\n",
    "### $Q(s_t, a_t) = max_\\pi R_{t+1}$\n",
    "\n",
    "$Q(s, a)$ ergibt, aus jetziger Sicht (dh. Zustand $s$), das bestmögliche Ergebnis am Ende des Spiels, wenn im Zustand $s$ die Aktion $a$ gewählt wird. Damit fällt die Wahl der optimalen Aktion leicht: die Aktion, die den höchsten Q-Wert ergibt:\n",
    "\n",
    "### $\\pi(s) = argmax_a Q(s, a)$\n",
    "\n",
    "Bleibt noch zu klären, wie man zu Q kommt. Wie beim *discounted future reward* lässt sich formulieren:\n",
    "\n",
    "### Bellman Equation: $Q(s, a) = r + \\gamma max_{a'} Q(s', a')$\n",
    "\n",
    "Das erscheint logisch: Die maximale zukünftige Belohnung für diesen Zustand und diese Aktion ist die unmittelbare Belohnung plus die maximale zukünftige Belohnung des nächsten Zustandes $s'$. Die maximalie zukünftige Belohnung wird ganz einfach bestimmt, indem für den nächsten Zustand $s'$ die Aktion $a'$ gewählt wird, für die es die größte Belohnung gibt.\n",
    "\n",
    "* Iterative Annäherung an die Q-Funktion mit Hilfe der Bellman Gleichung\n",
    "* Im einfachsten Fall werden die Q-Werte $Q[s', a']$ in einer Tabelle (*states* x *actions*) gehalten.\n",
    "* Algorithmus: solange, bis die Episode aus ist:\n",
    "  1. Aktion a setzen\n",
    "  2. Es ergeben sich Belohnung $r$ und Zustand $s'$\n",
    "  3. Update der Tabelle mit $Q[s, a] = Q[s, a] + \\alpha(r+\\gamma\\, max_{a'} Q[s', a'] - Q[s, a])$\n",
    "  4. $s = s'$\n",
    "* $\\alpha$ ist die Lernrate die bestimmt, wie sehr der bisherige Wert $Q[s,a]$ im neuen/aktualisierten $Q[s, a]$ Wert enthalten bleibt. Ist $\\alpha=1$ so neutralisieren sich die beiden $Q[a,s]$ und der bisherige Wert $Q[s,a]$ hat gar keinen Einfluss auf den aktuellen und es bleibt die oben genannte *Bellmann Equation*.\n",
    "* Zu Beginn des Trainings wird $max_{a'} Q[s', a']$ nichts Brauchbares liefern, da in der Q-Tabelle (sozusagen das Gedächtnis des *agents*) nach der Initialisierung noch nichts stehen kann, das mit der Umgebung in Zusammenhang steht. Die Werte werden jedoch mit jeder Iteration passender und werden, ausreichend Wiederholungen vorausgesetzt, gegen den wahren Q-Wert konvergieren.\n",
    "* Mit jeder Episode wird dazu gelernt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Umsetzungsmöglichkeiten: von der einfachen Tabelle zum Deep Q Network\n",
    "\n",
    "Einfaches Spiel - einfache Umgebung zB. Spielfeld mit 4x4 Feldern, Bewegungsmöglichkeit in vier Richtungen -> 16 Zustände, 4 Aktionen -> 16x4 Tabelle $Q$.\n",
    "\n",
    "Verallgemeinerung: Pixel + Farbabstufungen als Felder. Um Geschwindigkeiten zu erfassen 4 Screenshots. Jede Pixel-Farb-Layer-Kombination stellt einen Zustand dar. Bsp. 64x64 Bild mit 256 Graustufen und 4 Screen shots: $Farben^{res_x \\cdot res_y \\cdot layer} = 256^{64x64x4}$.\n",
    "\n",
    "Viele der auftretenden Zustände kommen nur selten vor, dh. selbst nur die besuchten Zustände in einer Tabelle zu berücksichtigen (sparse Table) ist wenig zielführend. Viele Zustände werden nie besucht.\n",
    "\n",
    "<hr>\n",
    "Kombinatorikhilfe:<br>\n",
    "Zahlensystem mit 256 Zuständen pro Stelle. So wie im Dezimalsystem 2 Stellen 100, also $10^2$ unterschiedliche Zustände ermöglichen, hat man im 256er System bei 2 Stellen $256^2$ Zustandsmöglichkeiten. Bei 64x64 Pixel, also 64x64 Stellen kommt man auf das gigantische Ergebnis von $256^{64 x 64 x 4}$ Zuständen, also um ein Vielfaches mehr als das Universum Atome hat.\n",
    "<hr>\n",
    "\n",
    "Weitere Verallgemeinerung - Sinnesorgane: Sehfeld, Gehör, Tastsinn, Geruchssinn\n",
    "\n",
    "Es hat hier auch ein Wechsel von der tatsächlichen Umgebung zur Wahrnehmung der Umgebung stattgefunden.\n",
    "\n",
    "Die $Q$-Funktion kann durch ein neuronales Netzwerk abgebildet werden, mit dem Zustand (4 Screens) und der Aktion als Input und dem Q-Wert als Ergebnis. Um in einem Schritt für alle Aktionen Q-Werte zu bekommen, kann alternativ der Zustand als Input genommen werden mit Q-Werten für jede Aktion als Ergebnis (verifizieren: DeepMind Ansatz).\n",
    "\n",
    "![](images/deep_q_learning.png \"\")\n",
    "Deep Q Learning<br>\n",
    "Source: selfmade\n",
    "\n",
    "Jeder Q-Wert ist ein reeller Wert. Die Fehlerfunktion, mit der die Differenz zwischen Zielwert ($r + \\gamma\\, max_{a'} Q(s', a')$) und Voraussage ($Q(s, a)$) minimiert werden soll, kann zB. so aussehen:\n",
    "\n",
    "###  $L = \\frac{1}{2}[r + \\gamma\\, max_{a'} Q(s', a') - Q(s, a)]^2$\n",
    "\n",
    "Abgeänderter Algorithmus für den Schritt von $s$ zu $s'$:\n",
    "* *feedforward pass*: füttern des NN mit dem derzeitigen Zustand um die Q-Werte für alle Aktionen zu bekommen.\n",
    "* *feedforward pass*: füttern des NN mit $s'$ und von den zurückgelieferten Q-Werten den größten berechnen:  $argmax_{a'} Q(s', a')$\n",
    "* damit den Zielwert $r + \\gamma\\, max_{a'} Q(s', a')$ für die Aktion $a$ berechnen. Für die nicht gewählten Aktionen wird der Zielwert auf den Wert, den das NN geliefert hat gesetzt, damit die Fehlerfunktion als Fehler $0$ berechnet.\n",
    "* Gewichte aktualisieren (backpropagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Netzwerk Topologie\n",
    "Was wäre eine passende Netzwerk-Topologie? Im DeepMind Atari Paper wurde folgender Aufbau gewählt:\n",
    "\n",
    "|Layer |Input|Filter size | Stride | Num filters | Activation | Output|\n",
    "|------|-----|------------|--------| ------------|------------|-------|\n",
    "|conv1\t|84x84x4\t|8×8  |\t4 |\t32\t|ReLU\t|20x20x32 |\n",
    "|conv2\t|20x20x32\t|4×4  |\t2 |\t64\t|ReLU\t|9x9x64   |\n",
    "|conv3\t|9x9x64\t    |3×3  |\t1 |\t64\t|ReLU\t|7x7x64   |\n",
    "|fc4\t|7x7x64\t\t|\t  |   |512  |ReLU\t|512      |\n",
    "|fc5\t|512\t\t|\t  |   |18\t|Linear\t|18       |\n",
    "\n",
    "Nachdem für diesen Anwendungsfall die Positionierung der erkannten Features wichtig ist, werden keine Pooling Layer verwendet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was hindert uns noch loszulegen? Nichts, allerdings wäre es wohl ein frustrierendes Unterfangen. Um die Q-Werte zum konvergieren zu bringen braucht es einige Tricks und eine Menge Rechenpower.\n",
    "\n",
    "## Experience Replay\n",
    "\n",
    "Aufeinanderfolgende Situationen  (*transitions*) sind oft recht ähnlich und führen mitunter zu lokalen Minima.\n",
    "\n",
    "Um das zu verhindern ist die wichtigste Technik ***experience replay***: Während des Spiels werden die Erfahrungen ($s, a, r, s'$) gespeichert (*replay memory*). Für das Trainieren des Netzwerkes werden anschließlich zufällig Werte aus dem *replay memory* genommen.\n",
    "\n",
    "Die Erfahrungen können auch von menschlichen Spielern gesammelt werden.\n",
    "Geht in Richtung *supervised learning*.\n",
    "\n",
    "## Exploration-Exploitation\n",
    "\n",
    "Die erstbeste erfolgreiche Strategie wird genommen und beibehalten weil sie den höchsten Q-Wert hat - *greedy*.\n",
    "\n",
    "Abhilfe:\n",
    "Anstatt immer die Aktion mit dem höchsten Q-Wert zu wählen, wird mit einer geringen Wahrscheinlichkeit $\\epsilon$ die Aktion zufällig gewählt: ***$\\epsilon$-greedy exploration***\n",
    "\n",
    "DeepMind hat $\\epsilon$ von $1$ beginnend während des Spielens bis $0.1$ abnehmen lassen. Zu Beginn werden so Werte gesammelt, der Zustands-Raum ausgelotet, gegen Ende hin geht es um das Konvergieren lassen der gefundenen Pfade.\n",
    "\n",
    "## Deep Q-Learning Algorithmus\n",
    "\n",
    "bis zu Spielende wiederholen:\n",
    "1. im Zustand $s$ eine Aktion $a$ wählen\n",
    "  * Mit Wahrscheinlichkeit $\\epsilon$ eine zufällige Aktion wählen\n",
    "  * andernfalls $a = argmax_{a'} Q(s, a')$\n",
    "2. Aktion $a$ durchführen\n",
    "3. *reward* $r$ und neuer Zustand $s'$ aus dem Environment (oder einem Interpreter) erfassen\n",
    "4. Erfahrung ($s, a, r, s'$) im *replay memory* $D$ speichern\n",
    "5. zufälligen Zustandwechsel aus dem *replay memory* wählen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
