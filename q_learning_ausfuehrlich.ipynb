{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning\n",
    "dt.: *Bestärkendes Lernen*\n",
    "\n",
    "Für wen?\n",
    "\n",
    "Schon einfache Computerspiele sind als Umgebung zu komplex um mit einfacher Klassifizierung zu einer brauchbaren Lösung zu kommen. Ursachen:\n",
    "\n",
    "* *supervised*: Jede Trainingsinstanz hat ein Label.\n",
    "  * Sogar in der einfachen Umgebung eines Computerspieles bräuchte man sehr viele Instanzen -> Screenshots für jede Situation/Spielzustand\n",
    "  * Spielzüge, die sich erst später im Spiel auswirken bleiben unberücksichtigt.\n",
    "\n",
    "\n",
    "* *Reinforcement Learning*: zwischen *supervised* und *unsupervised* *learning*. Manche Zustände bringen *rewards*. Diese Rewards können als (spärliche und zeitversetze) Labels betrachtet werden. Über *Reinforcement Learning* werden Wege dorthin durch Bewertung von Zuständen, die für sich genommen noch keinen Erfolg bringen, gelernt.\n",
    "\n",
    "### Schwierigkeiten:\n",
    "* **credit assignment problem**: Welche der Züge waren für das Bekommen der Belohnung verantwortlich?\n",
    "* **explore-exploit dilemma**: soll eine einmal gefundene, funktionierende Strategie beibehalten werden (*exploitation*), oder ist es zielführend eine nach einer besseren zu suchen (*exploration*)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Markov Decision Process (MDP)\n",
    "\n",
    "Eine übliche Art den Sachverhalt zu beschreiben ist die Betrachtung als *Markov Decision Process*.\n",
    "\n",
    "* Aktionen (*actions*) führen zu einem neuen Zustand (*state*) und können die Umgebung verändern.\n",
    "* *actions* führen manchmal zu Belohnungen durch das *environment* oder einen Interpreter des Zustands\n",
    "\n",
    "<img src=\"images/actor_state_environment.png\" alt=\"[Die Reinforcement Learning Komponenten: Actor State Environment\" style=\"width: 300px;\"/>\n",
    "Die Reinforcement Learning Komponenten: Actor State Environment<br>Source: selfmade\n",
    "\n",
    "* Die Regeln zur Wahl einer Aktion werden unter *policy* zusammengefasst.\n",
    "* Die Umgebung ist nicht zwingend deterministisch, dh. ausgehend vom gleichen Zustand führt eine Aktion nicht immer zum gleichen Folgezustand (Zufallselemente -> stochastisches Verhalten)\n",
    "* Eine *Episode* ist die Abfolge von *states*, *actions*, *rewards* bis zum Spielende (finaler Zustand $s_n$): $s_0,a_0,r_1, s_1,a_1,r_2, ...s_{n-1},a_{n-1},r_n,s_n$\n",
    "\n",
    "<img src=\"images/markov_decision_process.png\" alt=\"Markov Decision Process Graph\" style=\"width: 300px;\"/>\n",
    "Markov Decision Process Graph<br>Source: Wikipedia\n",
    "\n",
    "* Wichtige Eigenschaft von *MDP*: Der nächste Zustand $s_{i+1}$ (bzw. die Wahrscheinlichkeit dafür) hängt ausschließlich vom derzeitigen Zustand $s$ und der gewählten Aktion $a$ ab, nicht aber von den Zuständen davor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zukünftige Belohnungen diskontieren\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay\n",
    "\n",
    "Aufeinanderfolgende Situationen  (*transitions*) sind oft recht ähnlich und führen mitunter zu lokalen Minima.\n",
    "\n",
    "Um das zu verhindern ist die wichtigste Technik ***experience replay***: Während des Spiels werden die Erfahrungen ($s, a, r, s'$) gespeichert (*replay memory*). Für das Trainieren des Netzwerkes werden anschließlich zufällig Werte aus dem *replay memory* genommen.\n",
    "\n",
    "Die Erfahrungen können auch von menschlichen Spielern gesammelt werden.\n",
    "Geht in Richtung *supervised learning*.\n",
    "\n",
    "## Deep Q-Learning Algorithmus\n",
    "\n",
    "bis zu Spielende wiederholen:\n",
    "* im Zustand $s$ eine Aktion $a$ wählen\n",
    "  * Mit Wahrscheinlichkeit $\\epsilon$ eine zufällige Aktion wählen\n",
    "  * andernfalls $a = argmax_{a'} Q(s, a')$\n",
    "* Aktion $a$ durchführen\n",
    "* *reward* $r$ und neuer Zustand $s'$ aus dem Environment (oder einem Interpreter) erfassen\n",
    "* Erfahrung ($s, a, r, s'$) im *replay memory* $D$ speichern\n",
    "* zufälligen Zustandwechsel aus dem *replay memory* wählen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
